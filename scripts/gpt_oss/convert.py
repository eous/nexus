#!/usr/bin/env python3
"""
NEXUS: Neural Expert Unified Specialization
GPT-OSS Model Conversion

Convert GPT-OSS model by adding a shared expert to each layer.

This script transforms a standard GPT-OSS model (top-K MoE) into a 4+1 architecture
by adding one shared expert that is ALWAYS active alongside the top-K routed experts.

================================================================================
ARCHITECTURE TRANSFORMATION
================================================================================

BEFORE (Standard GPT-OSS):
    - Top-K routing: Select 4 experts per token
    - Only selected experts contribute to output
    - Total active: 4 experts per token

AFTER (4+1 Architecture):
    - Top-K routing: Select 4 routed experts per token
    - Shared expert: ALWAYS active for all tokens
    - Total active: 5 experts per token (4 routed + 1 shared)

Benefits:
    1. Shared expert learns common patterns across all tokens
    2. Routed experts can specialize more (shared handles common work)
    3. Can train shared expert while keeping routed experts frozen (Stage-0)
    4. MXFP4 quantized routed experts preserved (memory efficient)

================================================================================
INITIALIZATION STRATEGIES
================================================================================

random (RECOMMENDED):
    - Random initialization with proper scaling (config.initializer_range)
    - Uses improved lottery ticket fix for numerical stability
    - No router stats needed
    - Fast and effective
    - Best for general use

top1:
    - Copy the most-activated expert from each layer
    - Requires router stats from analyze_router_activations.py
    - Layer-specific initialization (different expert per layer)
    - Good when experts have clear specialization

top1_average:
    - Find globally most-activated expert across all layers
    - Copy that expert to all layers
    - Requires router stats
    - Consistent initialization across layers
    - Good for uniform starting point

top4_average:
    - Average the top-4 most activated experts per layer
    - Requires router stats
    - Smooths expert specialization
    - More robust than top1 (less sensitive to outliers)

pca_topK:
    - Average top-K experts selected by PCA importance analysis (K=4,8,16,24,32...)
    - Requires PCA stats from analyze_router_pca.py
    - Uses principal component analysis to identify diverse experts
    - More sophisticated than activation-based selection
    - Best for capturing expert complementarity
    - K determined by distribution analysis (default: 24 for even distributions)

================================================================================
ROUTER STATS FORMAT
================================================================================

JSON file generated by analyze_router_activations.py:

{
    "num_layers": 36,              # Number of transformer layers
    "num_experts": 128,            # Number of routed experts per layer
    "top_k": 4,                    # Top-k routing parameter
    "layers": {
        "layer_0": {
            "top_expert_idx": 42,                    # Most activated expert index
            "top_expert_count": 15234,               # Activation count
            "activation_counts": [cnt0, cnt1, ...]   # All 128 expert counts
        },
        "layer_1": { ... },
        ...
    }
}

Generate router stats:
    python analyze_router_activations.py \\
        --model /mnt/models/gpt-oss-120b \\
        --output router_stats.json \\
        --num-samples 1000

================================================================================
PCA STATS FORMAT
================================================================================

JSON file generated by analyze_router_pca.py:

{
    "method": "pca_top4",          # Analysis method identifier
    "num_layers": 36,              # Number of transformer layers
    "num_experts": 128,            # Number of routed experts per layer
    "layers": {
        "layer_0": {
            "top_expert_indices": [42, 7, 91, 15],    # PCA-selected experts
            "top_expert_scores": [0.15, 0.12, ...],   # PCA importance scores
            "importance_fraction": 0.68               # Coverage of total importance
        },
        "layer_1": { ... },
        ...
    }
}

Generate PCA stats:
    # Step 1: Collect router probabilities (1M tokens, nemotron-mixed default)
    python collect_router_probabilities.py \\
        --model /mnt/models/gpt-oss-120b \\
        --target-tokens 1000000 \\
        --dataset nemotron-mixed \\
        --output router_probs.npz

    # Step 2: PCA analysis (top-24 recommended for even distributions)
    python analyze_router_pca.py \\
        --input router_probs.npz \\
        --output pca_stats.json \\
        --top-k 24

================================================================================
USAGE EXAMPLES
================================================================================

Recommended (Random initialization):
    python convert_add_shared_expert.py \\
        --input /mnt/models/gpt-oss-120b \\
        --output /mnt/models/gpt-oss-120b-4plus1-base \\
        --init-strategy random

With router statistics:
    # Step 1: Generate router stats
    python analyze_router_activations.py \\
        --model /mnt/models/gpt-oss-120b \\
        --output router_stats.json

    # Step 2: Convert with stats-based initialization
    python convert_add_shared_expert.py \\
        --input /mnt/models/gpt-oss-120b \\
        --output /mnt/models/gpt-oss-120b-4plus1-base \\
        --router-stats router_stats.json \\
        --init-strategy top1_average

With PCA-guided initialization (recommended for optimal diversity):
    # Step 1: Collect router probabilities with distribution-aware sampling
    # Note: 1M tokens takes 3-6 hours; use 100K for testing
    python collect_router_probabilities.py \\
        --model /mnt/models/gpt-oss-120b \\
        --target-tokens 1000000 \\
        --dataset nemotron-mixed \\
        --output data/router_probs.npz

    # Step 2: Analyze with PCA and select top-24 diverse experts (recommended for even distributions)
    python analyze_router_pca.py \\
        --input data/router_probs.npz \\
        --output data/pca_stats.json \\
        --top-k 24

    # Step 3: Convert with PCA-guided initialization
    python convert_add_shared_expert.py \\
        --input /mnt/models/gpt-oss-120b \\
        --output /mnt/models/gpt-oss-120b-4plus1-pca \\
        --router-stats data/pca_stats.json \\
        --init-strategy pca_top24

Custom shared expert size:
    python convert_add_shared_expert.py \\
        --input /mnt/models/gpt-oss-120b \\
        --output /mnt/models/gpt-oss-120b-4plus1-custom \\
        --init-strategy random \\
        --shared-intermediate-size 4096

Dry run (preview without saving):
    python convert_add_shared_expert.py \\
        --input /mnt/models/gpt-oss-120b \\
        --output /mnt/models/gpt-oss-120b-4plus1-base \\
        --init-strategy random \\
        --dry-run

================================================================================
VALIDATION
================================================================================

The script automatically validates the converted model:
1. Shape validation during weight copying
2. Forward pass with test inputs
3. NaN/Inf detection in outputs
4. Router logits verification
5. Output shape and range checks

Additional validation recommended:
    python validate_model.py --model /mnt/models/gpt-oss-120b-4plus1-base

================================================================================
MEMORY REQUIREMENTS
================================================================================

Conversion process:
    - Model loading: ~70GB (MXFP4 quantized routed experts)
    - Shared expert init: ~1GB per expert
    - Model saving: ~70GB disk space
    - Peak memory: ~75GB (single GPU A100 80GB recommended)

The converted model preserves MXFP4 quantization for routed experts while adding
FP16/BF16 shared experts.

================================================================================
"""

# Standard library imports
import argparse
import json
import sys
import time
from pathlib import Path

# Third-party imports
import torch
import torch.nn as nn
from tqdm import tqdm

# HuggingFace imports
import transformers
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

# Import NEXUS shared expert implementation
# Ensure nexus package is installed: pip install -e /path/to/nexus
from nexus.models.gpt_oss import GptOssSharedExpert


def load_router_stats(stats_path: str) -> dict:
    """
    Load router activation statistics from JSON file.

    Expected format (generated by analyze_router_activations.py):
    {
        "num_layers": int,           # Number of transformer layers
        "num_experts": int,          # Number of routed experts per layer
        "top_k": int,                # Top-k routing parameter
        "layers": {
            "layer_N": {
                "top_expert_idx": int,              # Most activated expert index
                "top_expert_count": int,            # Activation count for top expert
                "activation_counts": List[int]      # Counts for all experts
            }
        }
    }

    Args:
        stats_path: Path to router statistics JSON file

    Returns:
        Dictionary containing router statistics

    Raises:
        FileNotFoundError: If stats_path doesn't exist
        json.JSONDecodeError: If file contains invalid JSON
        ValueError: If required keys are missing
    """
    try:
        with open(stats_path, "r") as f:
            stats = json.load(f)

        # Validate required top-level keys
        required_keys = ['num_layers', 'num_experts', 'layers']
        missing = [k for k in required_keys if k not in stats]
        if missing:
            raise ValueError(f"Router stats missing required keys: {missing}")

        return stats

    except FileNotFoundError:
        raise FileNotFoundError(f"Router stats file not found: {stats_path}")
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON in router stats file: {e}")


def initialize_shared_expert_from_top_expert(
    model,
    layer_idx: int,
    expert_idx: int,
    config,
) -> GptOssSharedExpert:
    """
    Initialize shared expert by copying weights from a specific routed expert.

    Args:
        model: Original model
        layer_idx: Layer index to copy from
        expert_idx: Expert index to copy from
        config: Model config with shared expert settings

    Returns:
        Initialized GptOssSharedExpert
    """
    print(f"  Initializing from layer {layer_idx}, expert {expert_idx}")

    # Get the source expert weights
    source_mlp = model.model.layers[layer_idx].mlp
    source_experts = source_mlp.experts

    # Create shared expert with correct dtype
    shared_intermediate_size = getattr(config, "shared_expert_intermediate_size", config.intermediate_size)
    shared_expert = GptOssSharedExpert(config, intermediate_size=shared_intermediate_size)

    # Convert to same dtype as model (typically bfloat16)
    # nn.Linear defaults to float32, but the model uses bfloat16
    model_dtype = next(model.parameters()).dtype
    shared_expert = shared_expert.to(dtype=model_dtype)

    # Copy weights from the selected expert
    # Note: GptOssExperts uses combined gate_up_proj with INTERLEAVED format
    #
    # WEIGHT FORMAT VERIFICATION:
    # The gate/up weights are stored in INTERLEAVED format: [g0, u0, g1, u1, g2, u2, ...]
    # This was verified by inspecting the official implementation in:
    # transformers/models/gpt_oss/modeling_gpt_oss.py line 124:
    #     gate, up = gate_up[..., ::2], gate_up[..., 1::2]
    # which uses even columns for gate and odd columns for up.
    #
    with torch.no_grad():
        # gate_up_proj shape: [num_experts, hidden_size, 2*expert_dim]
        # With USE_HUB_KERNELS=0, MXFP4 is auto-dequantized to BF16
        # Convert to FP32 for precise arithmetic
        gate_up_weight = source_experts.gate_up_proj[expert_idx].to(torch.float32)
        gate_up_bias = source_experts.gate_up_proj_bias[expert_idx].to(torch.float32)

        # Extract dimensions
        hidden_size = gate_up_weight.shape[0]
        expert_dim = gate_up_weight.shape[1] // 2

        # Split interleaved gate/up weights
        # Even columns (0, 2, 4, ...) are gate
        # Odd columns (1, 3, 5, ...) are up
        gate_weight = gate_up_weight[:, ::2]  # [hidden_size, expert_dim]
        up_weight = gate_up_weight[:, 1::2]    # [hidden_size, expert_dim]
        gate_bias = gate_up_bias[::2]          # [expert_dim]
        up_bias = gate_up_bias[1::2]           # [expert_dim]

        # Validate extracted shapes before copying
        assert gate_weight.shape == (hidden_size, expert_dim), \
            f"gate_weight shape {gate_weight.shape} != expected ({hidden_size}, {expert_dim})"
        assert up_weight.shape == (hidden_size, expert_dim), \
            f"up_weight shape {up_weight.shape} != expected ({hidden_size}, {expert_dim})"
        assert gate_bias.shape == (expert_dim,), \
            f"gate_bias shape {gate_bias.shape} != expected ({expert_dim},)"
        assert up_bias.shape == (expert_dim,), \
            f"up_bias shape {up_bias.shape} != expected ({expert_dim},)"

        # Transpose and copy to shared expert
        # nn.Linear weights are stored as [out_features, in_features]
        # so we transpose from [hidden_size, expert_dim] to [expert_dim, hidden_size]
        # Convert from FP32 to model dtype (typically BF16) during copy
        shared_expert.gate_proj.weight.copy_(gate_weight.T.to(model_dtype))
        shared_expert.gate_proj.bias.copy_(gate_bias.to(model_dtype))
        shared_expert.up_proj.weight.copy_(up_weight.T.to(model_dtype))
        shared_expert.up_proj.bias.copy_(up_bias.to(model_dtype))

        # Validate copied shapes
        assert shared_expert.gate_proj.weight.shape == (expert_dim, hidden_size), \
            f"gate_proj weight shape mismatch after copy"
        assert shared_expert.up_proj.weight.shape == (expert_dim, hidden_size), \
            f"up_proj weight shape mismatch after copy"

        # Copy down_proj (with USE_HUB_KERNELS=0, already BF16)
        down_weight = source_experts.down_proj[expert_idx].to(torch.float32)  # [expert_dim, hidden_size]
        down_bias = source_experts.down_proj_bias[expert_idx].to(torch.float32)  # [hidden_size]

        # Validate shapes
        assert down_weight.shape == (expert_dim, hidden_size), \
            f"down_weight shape {down_weight.shape} != expected ({expert_dim}, {hidden_size})"
        assert down_bias.shape == (hidden_size,), \
            f"down_bias shape {down_bias.shape} != expected ({hidden_size},)"

        # Transpose and copy (from [expert_dim, hidden_size] to [hidden_size, expert_dim])
        # Convert from FP32 to model dtype (typically BF16) during copy
        shared_expert.down_proj.weight.copy_(down_weight.T.to(model_dtype))
        shared_expert.down_proj.bias.copy_(down_bias.to(model_dtype))

        # Validate copied shape
        assert shared_expert.down_proj.weight.shape == (hidden_size, expert_dim), \
            f"down_proj weight shape mismatch after copy"

    return shared_expert


def initialize_shared_experts(
    model,
    router_stats: dict,
    config,
    init_strategy: str = "random",
) -> list:
    """
    Initialize shared experts for all layers using the specified strategy.

    This function handles all initialization strategies: random, top1, top1_average,
    and top4_average. The function name was changed from initialize_shared_expert_average
    because it handles multiple strategies, not just averaging.

    Args:
        model: Original GPT-OSS model (will extract weights from routed experts)
        router_stats: Router activation statistics (required for non-random/non-pca strategies, None for random/pca)
        config: Model configuration with shared expert settings
        init_strategy: Initialization strategy
            - "random": Random initialization with lottery ticket fix (recommended)
            - "top1": Copy most-activated expert from each layer (layer-specific)
            - "top1_average": Copy globally most-activated expert to all layers (uniform)
            - "top4_average": Average top-4 experts per layer (robust)
            - "pca_top4": Average top-4 experts selected by PCA importance (sophisticated)

    Returns:
        List of initialized GptOssSharedExpert instances (one per layer)

    Raises:
        ValueError: If unknown init_strategy or if router_stats missing when required
    """
    num_layers = model.config.num_hidden_layers
    shared_experts = []

    # Shared expert size used for all strategies
    shared_intermediate_size = getattr(
        config, "shared_expert_intermediate_size", config.intermediate_size
    )

    # For stats-based strategies, we require router_stats and matching dimension
    if init_strategy not in ["random", "pca_top4"]:
        if router_stats is None:
            raise ValueError(
                f"router_stats must be provided for init_strategy='{init_strategy}' "
                "(use --router-stats when running the conversion script)."
            )
        if shared_intermediate_size != config.intermediate_size:
            raise ValueError(
                f"init_strategy='{init_strategy}' requires shared_expert_intermediate_size "
                f"({shared_intermediate_size}) to match routed expert intermediate_size "
                f"({config.intermediate_size}). "
                "Use --init-strategy random if you want a different shared intermediate size."
            )

    # PCA strategy has its own validation (uses router_stats as pca_stats)
    if init_strategy.startswith("pca_top"):
        if router_stats is None:
            raise ValueError(
                f"{init_strategy} requires PCA stats from analyze_router_pca.py "
                "(use --router-stats to provide pca_stats.json)"
            )
        expected_method = router_stats.get("method", "")
        if not expected_method.startswith("pca_top"):
            raise ValueError(
                f"PCA stats appear to be from wrong analysis method. "
                f"Expected 'method': 'pca_topK', got: {expected_method}"
            )
        # PCA strategies can use different sizes (commonly 2× for better capacity)
        # No validation needed - user controls via --shared-intermediate-size

    if init_strategy == "random":
        # Random initialization with improved lottery ticket fix
        print("Initializing shared experts with random weights (improved lottery ticket fix)")
        # shared_intermediate_size already computed above

        # Standard deviation for initialization
        init_range = config.initializer_range  # 0.02 for gpt-oss

        total_zeros_fixed = 0
        total_params = 0

        # Get model dtype for consistent initialization
        model_dtype = next(model.parameters()).dtype

        for layer_idx in tqdm(range(num_layers), desc="Initializing shared experts (random)"):
            shared_expert = GptOssSharedExpert(config, intermediate_size=shared_intermediate_size)

            # Convert to model's dtype (typically bfloat16)
            # nn.Linear defaults to float32, but we need to match the model
            shared_expert = shared_expert.to(dtype=model_dtype)

            with torch.no_grad():
                # Initialize weights with normal distribution
                nn.init.normal_(shared_expert.gate_proj.weight, mean=0.0, std=init_range)
                nn.init.normal_(shared_expert.up_proj.weight, mean=0.0, std=init_range)
                nn.init.normal_(shared_expert.down_proj.weight, mean=0.0, std=init_range)

                # Improved lottery ticket fix: reinitialize only exact zeros
                # The lottery ticket hypothesis suggests sparse networks can train effectively,
                # but exact zeros can cause gradient issues. We only fix true zeros while
                # preserving the sparse structure (near-zero values are fine).
                for param_name, param in [
                    ('gate_proj', shared_expert.gate_proj.weight),
                    ('up_proj', shared_expert.up_proj.weight),
                    ('down_proj', shared_expert.down_proj.weight)
                ]:
                    # Only reinitialize exact zeros (not near-zero values)
                    zero_mask = (param == 0.0)
                    num_zeros = zero_mask.sum().item()

                    if num_zeros > 0:
                        # Reinitialize zeros with small random values (10% of normal init std)
                        param[zero_mask] = torch.randn(
                            num_zeros,
                            device=param.device,
                            dtype=param.dtype
                        ) * (init_range * 0.1)

                        total_zeros_fixed += num_zeros

                    total_params += param.numel()

                # Initialize biases to zero
                nn.init.zeros_(shared_expert.gate_proj.bias)
                nn.init.zeros_(shared_expert.up_proj.bias)
                nn.init.zeros_(shared_expert.down_proj.bias)

            shared_experts.append(shared_expert)

        print(f"  Initialized {num_layers} shared experts")
        print(f"    Weight init: Normal(mean=0, std={init_range})")
        print(f"    Bias init: Zeros")
        print(f"    Lottery ticket fix: {total_zeros_fixed}/{total_params} exact zeros reinitialized "
              f"({100*total_zeros_fixed/total_params:.4f}%)")
        print(f"    (Only exact zeros fixed with small random values, near-zeros preserved)")

    elif init_strategy == "top1":
        # Use top-1 expert from each layer
        print("Initializing shared experts using top-1 expert per layer")

        for layer_idx in tqdm(range(num_layers), desc="Initializing shared experts (top1)"):
            layer_stats = router_stats["layers"][f"layer_{layer_idx}"]
            top_expert_idx = layer_stats["top_expert_idx"]
            shared_expert = initialize_shared_expert_from_top_expert(
                model, layer_idx, top_expert_idx, config
            )
            # Note: initialize_shared_expert_from_top_expert already handles dtype conversion
            shared_experts.append(shared_expert)

    elif init_strategy == "top1_average":
        # Find globally most-activated expert across all layers
        print("Initializing shared experts using globally most-activated expert")

        # Find most activated expert globally
        max_count = 0
        best_layer = 0
        best_expert = 0

        for layer_idx in range(num_layers):
            layer_stats = router_stats["layers"][f"layer_{layer_idx}"]
            if layer_stats["top_expert_count"] > max_count:
                max_count = layer_stats["top_expert_count"]
                best_layer = layer_idx
                best_expert = layer_stats["top_expert_idx"]

        print(f"  Global top expert: layer {best_layer}, expert {best_expert} "
              f"(count: {max_count})")

        # Use this expert for all layers
        base_shared_expert = initialize_shared_expert_from_top_expert(
            model, best_layer, best_expert, config
        )

        # Replicate for all layers (direct weight copy is more efficient than state_dict)
        for layer_idx in tqdm(range(num_layers), desc="Replicating shared expert to all layers"):
            # Create a new instance
            shared_expert = GptOssSharedExpert(
                config,
                intermediate_size=getattr(config, "shared_expert_intermediate_size", config.intermediate_size)
            )

            # Convert to same dtype as base expert (which is already correct dtype from initialize_shared_expert_from_top_expert)
            shared_expert = shared_expert.to(dtype=base_shared_expert.gate_proj.weight.dtype)

            # Direct weight copy (more efficient than load_state_dict for this use case)
            with torch.no_grad():
                shared_expert.gate_proj.weight.copy_(base_shared_expert.gate_proj.weight)
                shared_expert.gate_proj.bias.copy_(base_shared_expert.gate_proj.bias)
                shared_expert.up_proj.weight.copy_(base_shared_expert.up_proj.weight)
                shared_expert.up_proj.bias.copy_(base_shared_expert.up_proj.bias)
                shared_expert.down_proj.weight.copy_(base_shared_expert.down_proj.weight)
                shared_expert.down_proj.bias.copy_(base_shared_expert.down_proj.bias)

            shared_experts.append(shared_expert)

    elif init_strategy == "top4_average":
        # Average top-K experts per layer (typically K=4, but adaptive to available experts)
        print("Initializing shared experts by averaging top-K most activated experts per layer")

        # Get model dtype for consistent initialization
        model_dtype = next(model.parameters()).dtype

        for layer_idx in tqdm(range(num_layers), desc="Averaging top-K experts per layer"):
            layer_stats = router_stats["layers"][f"layer_{layer_idx}"]

            # Get top-K expert indices (adaptive K based on available experts)
            activation_counts = torch.tensor(layer_stats["activation_counts"])
            k = min(4, len(activation_counts))  # Use top-4 or fewer if not enough experts

            if k == 0:
                raise ValueError(f"Layer {layer_idx}: No activation counts available")

            top_k_indices = torch.topk(activation_counts, k=k).indices.tolist()

            print(f"  Layer {layer_idx}: averaging top-{k} experts {top_k_indices}")

            # Initialize shared expert
            shared_intermediate_size = getattr(config, "shared_expert_intermediate_size", config.intermediate_size)
            shared_expert = GptOssSharedExpert(config, intermediate_size=shared_intermediate_size)

            # Convert to model's dtype (typically bfloat16)
            shared_expert = shared_expert.to(dtype=model_dtype)

            # Average weights from top-K experts
            source_experts = model.model.layers[layer_idx].mlp.experts

            with torch.no_grad():
                # Initialize accumulators (sums, will be averaged by k) in FP32
                gate_weight_sum = torch.zeros_like(shared_expert.gate_proj.weight, dtype=torch.float32)
                gate_bias_sum = torch.zeros_like(shared_expert.gate_proj.bias, dtype=torch.float32)
                up_weight_sum = torch.zeros_like(shared_expert.up_proj.weight, dtype=torch.float32)
                up_bias_sum = torch.zeros_like(shared_expert.up_proj.bias, dtype=torch.float32)
                down_weight_sum = torch.zeros_like(shared_expert.down_proj.weight, dtype=torch.float32)
                down_bias_sum = torch.zeros_like(shared_expert.down_proj.bias, dtype=torch.float32)

                # Accumulate from top-K experts
                for expert_idx in top_k_indices:
                    # With USE_HUB_KERNELS=0, experts are BF16, convert to FP32 for averaging
                    gate_up_weight = source_experts.gate_up_proj[expert_idx].to(torch.float32)
                    gate_up_bias = source_experts.gate_up_proj_bias[expert_idx].to(torch.float32)

                    # Split interleaved gate/up (see citation in initialize_shared_expert_from_top_expert)
                    gate_weight = gate_up_weight[:, ::2].T  # [expert_dim, hidden_size]
                    up_weight = gate_up_weight[:, 1::2].T
                    gate_bias = gate_up_bias[::2]
                    up_bias = gate_up_bias[1::2]

                    gate_weight_sum += gate_weight
                    gate_bias_sum += gate_bias
                    up_weight_sum += up_weight
                    up_bias_sum += up_bias

                    # With USE_HUB_KERNELS=0, experts are BF16, convert to FP32 for averaging
                    down_weight = source_experts.down_proj[expert_idx].to(torch.float32).T
                    down_bias = source_experts.down_proj_bias[expert_idx].to(torch.float32)

                    down_weight_sum += down_weight
                    down_bias_sum += down_bias

                # Average (in FP32) and convert to model dtype (BF16) when copying
                shared_expert.gate_proj.weight.copy_((gate_weight_sum / float(k)).to(model_dtype))
                shared_expert.gate_proj.bias.copy_((gate_bias_sum / float(k)).to(model_dtype))
                shared_expert.up_proj.weight.copy_((up_weight_sum / float(k)).to(model_dtype))
                shared_expert.up_proj.bias.copy_((up_bias_sum / float(k)).to(model_dtype))
                shared_expert.down_proj.weight.copy_((down_weight_sum / float(k)).to(model_dtype))
                shared_expert.down_proj.bias.copy_((down_bias_sum / float(k)).to(model_dtype))

            shared_experts.append(shared_expert)

    elif init_strategy.startswith("pca_top"):
        # Average top-K experts per layer selected by PCA importance analysis
        # Extract K from the method in router_stats
        method = router_stats.get("method", "pca_top4")
        k_from_method = int(method.replace("pca_top", ""))
        print(f"Initializing shared experts using PCA-selected top-{k_from_method} experts per layer")

        # Get model dtype for consistent initialization
        model_dtype = next(model.parameters()).dtype

        for layer_idx in tqdm(range(num_layers), desc=f"Averaging PCA-selected top-{k_from_method} experts per layer"):
            layer_stats = router_stats["layers"][f"layer_{layer_idx}"]

            # Get PCA-selected top-K expert indices
            top_expert_indices = layer_stats["top_expert_indices"]
            top_expert_scores = layer_stats.get("top_expert_scores", [1.0] * len(top_expert_indices))

            if len(top_expert_indices) != k_from_method:
                print(f"  WARNING: Layer {layer_idx} has {len(top_expert_indices)} experts (expected {k_from_method})")

            print(f"  Layer {layer_idx}: PCA-selected experts {top_expert_indices} "
                  f"(scores: {[f'{s:.3f}' for s in top_expert_scores]})")

            # Initialize shared expert
            shared_intermediate_size = getattr(config, "shared_expert_intermediate_size", config.intermediate_size)
            shared_expert = GptOssSharedExpert(config, intermediate_size=shared_intermediate_size)

            # Convert to model's dtype (typically bfloat16)
            shared_expert = shared_expert.to(dtype=model_dtype)

            # Average weights from PCA-selected experts (equal weighting for simplicity)
            source_experts = model.model.layers[layer_idx].mlp.experts
            k = len(top_expert_indices)

            with torch.no_grad():
                # Initialize accumulators (sums, will be averaged by k) in FP32
                gate_weight_sum = torch.zeros_like(shared_expert.gate_proj.weight, dtype=torch.float32)
                gate_bias_sum = torch.zeros_like(shared_expert.gate_proj.bias, dtype=torch.float32)
                up_weight_sum = torch.zeros_like(shared_expert.up_proj.weight, dtype=torch.float32)
                up_bias_sum = torch.zeros_like(shared_expert.up_proj.bias, dtype=torch.float32)
                down_weight_sum = torch.zeros_like(shared_expert.down_proj.weight, dtype=torch.float32)
                down_bias_sum = torch.zeros_like(shared_expert.down_proj.bias, dtype=torch.float32)

                # Accumulate from PCA-selected experts
                for expert_idx in top_expert_indices:
                    # With USE_HUB_KERNELS=0, experts are BF16, convert to FP32 for averaging
                    gate_up_weight = source_experts.gate_up_proj[expert_idx].to(torch.float32)
                    gate_up_bias = source_experts.gate_up_proj_bias[expert_idx].to(torch.float32)

                    # Split interleaved gate/up (see citation in initialize_shared_expert_from_top_expert)
                    gate_weight = gate_up_weight[:, ::2].T  # [expert_dim, hidden_size]
                    up_weight = gate_up_weight[:, 1::2].T
                    gate_bias = gate_up_bias[::2]
                    up_bias = gate_up_bias[1::2]

                    gate_weight_sum += gate_weight
                    gate_bias_sum += gate_bias
                    up_weight_sum += up_weight
                    up_bias_sum += up_bias

                    # With USE_HUB_KERNELS=0, experts are BF16, convert to FP32 for averaging
                    down_weight = source_experts.down_proj[expert_idx].to(torch.float32).T
                    down_bias = source_experts.down_proj_bias[expert_idx].to(torch.float32)

                    down_weight_sum += down_weight
                    down_bias_sum += down_bias

                # Average (in FP32) and convert to model dtype (BF16) when copying
                shared_expert.gate_proj.weight.copy_((gate_weight_sum / float(k)).to(model_dtype))
                shared_expert.gate_proj.bias.copy_((gate_bias_sum / float(k)).to(model_dtype))
                shared_expert.up_proj.weight.copy_((up_weight_sum / float(k)).to(model_dtype))
                shared_expert.up_proj.bias.copy_((up_bias_sum / float(k)).to(model_dtype))
                shared_expert.down_proj.weight.copy_((down_weight_sum / float(k)).to(model_dtype))
                shared_expert.down_proj.bias.copy_((down_bias_sum / float(k)).to(model_dtype))

            shared_experts.append(shared_expert)

    else:
        raise ValueError(f"Unknown init_strategy: {init_strategy}")

    return shared_experts


def add_shared_experts_to_model(model, shared_experts: list, config):
    """
    Add shared experts to all MoE layers in the model.

    Args:
        model: Original model
        shared_experts: List of initialized shared experts
        config: Updated config with shared expert settings

    Raises:
        RuntimeError: If moving shared expert to device fails (e.g., OOM)
    """
    print("\nAdding shared experts to model layers...")

    num_layers = model.config.num_hidden_layers

    if len(shared_experts) != num_layers:
        raise ValueError(
            f"Number of shared experts ({len(shared_experts)}) != num_layers ({num_layers})"
        )

    # Check if shared experts already exist (model may have been converted before)
    first_mlp = model.model.layers[0].mlp
    if hasattr(first_mlp, 'shared_expert') and first_mlp.shared_expert is not None:
        print("\n" + "="*80)
        print("⚠️  WARNING: Model already has shared experts!")
        print("="*80)
        print("This model appears to have been converted before.")
        print("Continuing will overwrite existing shared experts with new initialization.")
        print("\n⚠️  This is typically NOT what you want - you may lose trained weights!")
        print("\nIf you want to:")
        print("  - Convert again: Answer 'y' (will overwrite)")
        print("  - Cancel: Answer 'n' or Ctrl+C")
        print("="*80)

        try:
            response = input("\nContinue and overwrite existing shared experts? (y/n): ")
            if response.lower() != 'y':
                print("\nConversion cancelled.")
                sys.exit(0)
            print("\nOverwriting existing shared experts...")
        except KeyboardInterrupt:
            print("\n\nConversion cancelled by user.")
            sys.exit(0)
        print()

    for layer_idx in tqdm(range(num_layers), desc="Adding shared experts to model layers"):
        mlp = model.model.layers[layer_idx].mlp

        # Add shared expert to this layer
        # Move to same device as model
        try:
            shared_experts[layer_idx] = shared_experts[layer_idx].to(model.device)
            mlp.shared_expert = shared_experts[layer_idx]
            mlp.num_shared_experts = config.num_shared_experts

        except RuntimeError as e:
            if "out of memory" in str(e).lower():
                print(f"\n✗ ERROR: Out of memory when moving shared expert to {model.device}")
                print(f"  Failed at layer {layer_idx}/{num_layers}")
                print(f"\nSuggestions:")
                print(f"  1. Use --device cpu (slower but uses RAM instead of VRAM)")
                print(f"  2. Use smaller --shared-intermediate-size")
                print(f"  3. Clear GPU cache before running")
                print(f"  4. Close other GPU processes")
            raise

    print(f"\n  ✓ Added shared experts to all {num_layers} layers")


def main():
    parser = argparse.ArgumentParser(description="Add shared expert to gpt-oss model")
    parser.add_argument(
        "--input",
        type=str,
        default="/mnt/models/gpt-oss-120b",
        help="Path to input gpt-oss model",
    )
    parser.add_argument(
        "--output",
        type=str,
        default="models/gpt-oss-120b-4plus1-base",
        help="Path to output modified model",
    )
    parser.add_argument(
        "--router-stats",
        type=str,
        default=None,
        help="Path to router statistics JSON (only needed for non-random init)",
    )
    parser.add_argument(
        "--init-strategy",
        type=str,
        default="random",
        help="Shared expert initialization strategy: random, top1, top1_average, top4_average, pca_top4, pca_top8, pca_top16, pca_top24, pca_top32, etc. (default: random)",
    )
    parser.add_argument(
        "--num-shared-experts",
        type=int,
        default=1,
        help="Number of shared experts to add",
    )
    parser.add_argument(
        "--shared-intermediate-size",
        type=int,
        default=None,
        help="Intermediate size for shared expert (default: same as routed)",
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda" if torch.cuda.is_available() else "cpu",
        help="Device to use for conversion",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=False,
        help="Preview conversion without saving (default: False)",
    )

    args = parser.parse_args()

    # Validate arguments
    # Validate init_strategy
    valid_strategies = ["random", "top1", "top1_average", "top4_average"]
    if args.init_strategy not in valid_strategies and not args.init_strategy.startswith("pca_top"):
        parser.error(f"Invalid --init-strategy: {args.init_strategy}. "
                    f"Must be one of {valid_strategies} or pca_topK (e.g., pca_top4, pca_top24)")

    if args.num_shared_experts < 0:
        parser.error("--num-shared-experts must be >= 0")

    # Current implementation supports at most a single shared expert per layer
    if args.num_shared_experts > 1:
        parser.error("--num-shared-experts > 1 is not currently supported; use 0 or 1.")

    if args.num_shared_experts == 0:
        print("WARNING: --num-shared-experts=0: no shared experts will be added.")
        print("The output model will be architecturally identical to the input.")
        print("Continue anyway? (y/n): ", end="")
        response = input()
        if response.lower() != 'y':
            print("Conversion cancelled.")
            sys.exit(0)

    print("=" * 80)
    print("GPT-OSS Shared Expert Conversion")
    print("=" * 80)
    print(f"Input model: {args.input}")
    print(f"Output model: {args.output}")
    print(f"Router stats: {args.router_stats}")
    print(f"Init strategy: {args.init_strategy}")
    print(f"Device: {args.device}")
    if args.dry_run:
        print(f"Mode: DRY RUN (no files will be saved)")
    print("=" * 80)

    # Validate GPU availability and device selection
    if 'cuda' in args.device:
        if not torch.cuda.is_available():
            raise RuntimeError(
                f"CUDA device '{args.device}' specified but CUDA is not available. "
                "Check your PyTorch installation and GPU drivers, or use --device cpu"
            )

        num_gpus = torch.cuda.device_count()
        device_id = int(args.device.split(':')[1]) if ':' in args.device else 0

        if device_id >= num_gpus:
            raise ValueError(
                f"Device '{args.device}' not available (only {num_gpus} GPU(s) detected). "
                f"Valid devices: cuda:0 through cuda:{num_gpus-1}"
            )

        print(f"\n✓ GPU validation passed: {num_gpus} GPU(s) available")
        print(f"  Will use: {args.device}")
    else:
        print(f"\n  Using CPU for conversion (slower but no GPU required)")

    # Load router statistics (only needed for non-random init)
    router_stats = None
    if args.init_strategy != "random":
        if args.router_stats is None:
            raise ValueError(f"--router-stats required for init_strategy={args.init_strategy}")
        print("\nLoading router statistics...")
        router_stats = load_router_stats(args.router_stats)
        print(f"Loaded stats for {router_stats['num_layers']} layers, "
              f"{router_stats['num_experts']} experts per layer")
    else:
        print("\nUsing random initialization (no router stats needed)")

    # Check and report quantization configuration before loading
    print("\nChecking model configuration...")
    config_path = Path(args.input) / "config.json"

    if config_path.exists():
        with open(config_path) as f:
            config_json = json.load(f)

        quantization_config = config_json.get("quantization_config", {})
        quant_method = quantization_config.get("quant_method")

        if quant_method == "mxfp4":
            print("  ✓ Model uses MXFP4 quantization (4-bit routed experts)")
            print("    Routed experts will remain quantized (memory efficient)")
            print("    Shared expert will be FP16/BF16 (trainable)")
        elif quant_method:
            print(f"  ℹ Model uses {quant_method} quantization")
            print(f"    Routed experts will use {quant_method}")
            print("    Shared expert will be FP16/BF16")
        else:
            print("  ℹ Model has no quantization (full precision)")
            print("    All weights will be FP16/BF16")
    else:
        print("  ⚠ Warning: Could not find config.json, cannot verify quantization")

    # For PCA strategies: Load on CPU, compute weights, then reload with MXFP4
    # This preserves MXFP4 quantization in final model
    if args.init_strategy.startswith("pca_top"):
        print("\n" + "="*80)
        print("PCA STRATEGY: Two-Phase Approach")
        print("="*80)
        print("Phase 1: Load donor model on CPU, compute averaged shared expert weights")
        print("Phase 2: Load MXFP4 model, attach shared expert, save (~59GB vs ~81GB)")
        print("="*80)

        # PHASE 1: Compute shared expert weights on CPU
        print("\nPHASE 1: Computing shared expert weights on CPU...")
        print("  Loading donor model (dequantized, ~250GB RAM)")

        from transformers.utils.quantization_config import Mxfp4Config
        quant_config = Mxfp4Config(dequantize=True)

        donor_model = AutoModelForCausalLM.from_pretrained(
            args.input,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            quantization_config=quant_config,
            device_map="cpu",
            low_cpu_mem_usage=True,
        )

        print("  ✓ Donor model loaded on CPU (dequantized)")

        # Compute shared expert weights from donor model
        print("\n  Computing averaged shared expert weights...")
        print(f"  Averaging top-{router_stats.get('method', 'pca_top24').replace('pca_top', '')} experts per layer")

        # Store weights for later (will attach after reloading MXFP4 model)
        num_layers = donor_model.config.num_hidden_layers
        shared_expert_weights_cpu = []

        # Determine shared expert size
        shared_intermediate_size = (
            args.shared_intermediate_size
            if args.shared_intermediate_size is not None
            else donor_model.config.intermediate_size
        )

        # Create temporary config for shared expert template
        temp_config = donor_model.config
        temp_config.shared_expert_intermediate_size = shared_intermediate_size

        for layer_idx in tqdm(range(num_layers), desc="Computing shared expert weights"):
            layer_stats = router_stats["layers"][f"layer_{layer_idx}"]
            top_expert_indices = layer_stats["top_expert_indices"]

            source_experts = donor_model.model.layers[layer_idx].mlp.experts

            # Create template to get shapes
            template = GptOssSharedExpert(temp_config, intermediate_size=shared_intermediate_size)

            # Accumulators in FP32
            gate_weight_sum = torch.zeros_like(template.gate_proj.weight, dtype=torch.float32)
            gate_bias_sum = torch.zeros_like(template.gate_proj.bias, dtype=torch.float32)
            up_weight_sum = torch.zeros_like(template.up_proj.weight, dtype=torch.float32)
            up_bias_sum = torch.zeros_like(template.up_proj.bias, dtype=torch.float32)
            down_weight_sum = torch.zeros_like(template.down_proj.weight, dtype=torch.float32)
            down_bias_sum = torch.zeros_like(template.down_proj.bias, dtype=torch.float32)

            k = len(top_expert_indices)

            for expert_idx in top_expert_indices:
                gate_up_weight = source_experts.gate_up_proj[expert_idx].to(torch.float32)
                gate_up_bias = source_experts.gate_up_proj_bias[expert_idx].to(torch.float32)

                gate_weight = gate_up_weight[:, ::2].T
                up_weight = gate_up_weight[:, 1::2].T
                gate_bias = gate_up_bias[::2]
                up_bias = gate_up_bias[1::2]

                gate_weight_sum += gate_weight
                gate_bias_sum += gate_bias
                up_weight_sum += up_weight
                up_bias_sum += up_bias

                down_weight = source_experts.down_proj[expert_idx].to(torch.float32).T
                down_bias = source_experts.down_proj_bias[expert_idx].to(torch.float32)

                down_weight_sum += down_weight
                down_bias_sum += down_bias

            # Store averaged weights (keep in FP32 for now, will convert when attaching)
            shared_expert_weights_cpu.append({
                'gate_proj.weight': gate_weight_sum / float(k),
                'gate_proj.bias': gate_bias_sum / float(k),
                'up_proj.weight': up_weight_sum / float(k),
                'up_proj.bias': up_bias_sum / float(k),
                'down_proj.weight': down_weight_sum / float(k),
                'down_proj.bias': down_bias_sum / float(k),
            })

        print(f"  ✓ Computed shared expert weights for {num_layers} layers")

        # CRITICAL: Delete donor model to free RAM before loading MXFP4
        print("\n  Unloading donor model to free RAM...")
        del donor_model
        import gc
        gc.collect()
        torch.cuda.empty_cache() if torch.cuda.is_available() else None
        print("  ✓ Donor model unloaded")

        # PHASE 2: Load original MXFP4 model
        print("\nPHASE 2: Loading original MXFP4 model...")
        print("  Preserving MXFP4 quantization (4-bit routed experts)")

        model = AutoModelForCausalLM.from_pretrained(
            args.input,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map=args.device,
        )

        print("  ✓ MXFP4 model loaded (quantization preserved)")

    else:
        # Non-PCA strategies: Load normally on GPU
        print("\nLoading original model...")

        model = AutoModelForCausalLM.from_pretrained(
            args.input,
            trust_remote_code=True,
            torch_dtype=torch.bfloat16,
            device_map=args.device,
        )

        shared_expert_weights_cpu = None  # Not used for non-PCA

    # Load tokenizer with validation
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            args.input,
            trust_remote_code=True,
        )

        # Validate tokenizer matches model
        if tokenizer.vocab_size != model.config.vocab_size:
            print(f"\n⚠️  WARNING: Tokenizer vocab size ({tokenizer.vocab_size}) != "
                  f"model vocab size ({model.config.vocab_size})")
            print("  This may cause issues during training/inference")

        print(f"  ✓ Tokenizer loaded (vocab_size: {tokenizer.vocab_size})")

    except Exception as e:
        print(f"\n✗ ERROR: Failed to load tokenizer: {e}")
        print("  Model conversion can continue, but you'll need to add a tokenizer later")
        tokenizer = None

    print(f"\nModel loaded: {model.config.num_hidden_layers} layers (MXFP4 routed experts preserved)")

    # Validate router stats match model architecture (if using stats-based init)
    if router_stats is not None:
        print("\nValidating router stats against model architecture...")

        if router_stats['num_layers'] != model.config.num_hidden_layers:
            raise ValueError(
                f"Router stats num_layers ({router_stats['num_layers']}) != "
                f"model num_layers ({model.config.num_hidden_layers})"
            )

        if router_stats['num_experts'] != model.config.num_local_experts:
            raise ValueError(
                f"Router stats num_experts ({router_stats['num_experts']}) != "
                f"model num_local_experts ({model.config.num_local_experts})"
            )

        # Different validation for PCA stats vs activation stats
        is_pca_stats = router_stats.get('method', '').startswith('pca_top')

        if is_pca_stats:
            # Validate PCA stats
            for layer_idx in range(router_stats['num_layers']):
                layer_key = f"layer_{layer_idx}"
                if layer_key not in router_stats['layers']:
                    raise ValueError(f"Missing {layer_key} in PCA stats")

                layer_stats = router_stats['layers'][layer_key]
                required_keys = ['top_expert_indices', 'top_expert_scores']
                missing_keys = [k for k in required_keys if k not in layer_stats]
                if missing_keys:
                    raise ValueError(f"Layer {layer_idx} missing required PCA keys: {missing_keys}")

                # Validate expert indices are within bounds
                for expert_idx in layer_stats['top_expert_indices']:
                    if not (0 <= expert_idx < router_stats['num_experts']):
                        raise ValueError(
                            f"Layer {layer_idx}: expert index {expert_idx} out of range for "
                            f"num_experts={router_stats['num_experts']}"
                        )

            print(f"  ✓ PCA stats validation passed")
            print(f"    {router_stats['num_layers']} layers × {router_stats['num_experts']} experts")
            print(f"    Method: {router_stats['method']}")

        else:
            # Validate activation-based router stats
            for layer_idx in range(router_stats['num_layers']):
                layer_key = f"layer_{layer_idx}"
                if layer_key not in router_stats['layers']:
                    raise ValueError(f"Missing {layer_key} in router stats")

                layer_stats = router_stats['layers'][layer_key]
                required_keys = ['top_expert_idx', 'top_expert_count', 'activation_counts']
                missing_keys = [k for k in required_keys if k not in layer_stats]
                if missing_keys:
                    raise ValueError(f"Layer {layer_idx} missing required keys: {missing_keys}")

                # Validate activation counts length matches num_experts
                if len(layer_stats['activation_counts']) != router_stats['num_experts']:
                    raise ValueError(
                        f"Layer {layer_idx}: activation_counts length ({len(layer_stats['activation_counts'])}) "
                        f"!= num_experts ({router_stats['num_experts']})"
                    )

                # Validate top_expert_idx is within bounds
                top_idx = layer_stats["top_expert_idx"]
                if not (0 <= top_idx < router_stats["num_experts"]):
                    raise ValueError(
                        f"Layer {layer_idx}: top_expert_idx ({top_idx}) out of range for "
                        f"num_experts={router_stats['num_experts']}"
                    )

            print(f"  ✓ Router stats validation passed")
            print(f"    {router_stats['num_layers']} layers × {router_stats['num_experts']} experts")

    # Update config for shared experts
    print("\nUpdating model config...")
    model.config.num_shared_experts = args.num_shared_experts
    model.config.shared_expert_intermediate_size = (
        args.shared_intermediate_size
        if args.shared_intermediate_size is not None
        else model.config.intermediate_size
    )

    print(f"  num_shared_experts: {model.config.num_shared_experts}")
    print(f"  shared_expert_intermediate_size: {model.config.shared_expert_intermediate_size}")

    if args.num_shared_experts > 0:
        # Initialize shared experts
        print("\nInitializing shared experts...")

        # For PCA with pre-computed weights, attach them directly
        if args.init_strategy.startswith("pca_top") and shared_expert_weights_cpu is not None:
            print("  Using pre-computed weights from Phase 1")

            shared_experts = []
            model_dtype = next(model.parameters()).dtype

            for layer_idx in tqdm(range(len(shared_expert_weights_cpu)), desc="Attaching shared experts"):
                # Create shared expert
                shared_expert = GptOssSharedExpert(
                    model.config,
                    intermediate_size=model.config.shared_expert_intermediate_size
                )
                shared_expert = shared_expert.to(dtype=model_dtype)

                # Load pre-computed weights (convert FP32 → model dtype)
                layer_weights = shared_expert_weights_cpu[layer_idx]
                with torch.no_grad():
                    shared_expert.gate_proj.weight.copy_(layer_weights['gate_proj.weight'].to(model_dtype))
                    shared_expert.gate_proj.bias.copy_(layer_weights['gate_proj.bias'].to(model_dtype))
                    shared_expert.up_proj.weight.copy_(layer_weights['up_proj.weight'].to(model_dtype))
                    shared_expert.up_proj.bias.copy_(layer_weights['up_proj.bias'].to(model_dtype))
                    shared_expert.down_proj.weight.copy_(layer_weights['down_proj.weight'].to(model_dtype))
                    shared_expert.down_proj.bias.copy_(layer_weights['down_proj.bias'].to(model_dtype))

                shared_experts.append(shared_expert)

            print(f"  ✓ Attached {len(shared_experts)} pre-computed shared experts")

        else:
            # Normal initialization for non-PCA strategies
            shared_experts = initialize_shared_experts(
                model=model,
                router_stats=router_stats,
                config=model.config,
                init_strategy=args.init_strategy,
            )

        # Add shared experts to model
        add_shared_experts_to_model(model, shared_experts, model.config)
    else:
        print("\nNo shared experts requested (--num-shared-experts=0); "
              "skipping shared-expert initialization and attachment.")

    # Validate forward pass with comprehensive checks
    print("\nValidating model with comprehensive forward pass...")
    validation_passed = False

    try:
        # Test with realistic sequence length (not just 10 tokens)
        batch_size, seq_len = 2, 128

        # Use tokenizer vocab_size if available, otherwise fall back to model config
        vocab_size = getattr(tokenizer, "vocab_size", None)
        if vocab_size is None:
            vocab_size = model.config.vocab_size
            print(f"  Using model config vocab_size: {vocab_size} (tokenizer not available)")

        if not isinstance(vocab_size, int) or vocab_size <= 0:
            raise ValueError(
                f"Invalid vocab_size for dummy input generation: {vocab_size}. "
                "Check tokenizer/model configuration."
            )

        dummy_input = torch.randint(0, min(vocab_size, 50000), (batch_size, seq_len)).to(args.device)

        print(f"  Test input shape: {dummy_input.shape}")

        with torch.no_grad():
            outputs = model(dummy_input, output_router_logits=True)

        # Check for NaN/Inf in outputs
        if torch.isnan(outputs.logits).any():
            raise ValueError("Output logits contain NaN values - weight initialization may be incorrect")
        if torch.isinf(outputs.logits).any():
            raise ValueError("Output logits contain Inf values - numerical instability detected")

        # Verify output shape
        expected_shape = (batch_size, seq_len, model.config.vocab_size)
        if outputs.logits.shape != expected_shape:
            raise ValueError(
                f"Output shape mismatch: expected {expected_shape}, got {outputs.logits.shape}"
            )

        # Check router logits (verify routing still works)
        if hasattr(outputs, 'router_logits') and outputs.router_logits is not None:
            num_router_layers = len(outputs.router_logits)
            print(f"  ✓ Router logits available: {num_router_layers} layers")

            # Validate router logits don't have NaN/Inf
            for i, logits in enumerate(outputs.router_logits):
                if torch.isnan(logits).any():
                    raise ValueError(f"Router logits layer {i} contains NaN values")
                if torch.isinf(logits).any():
                    raise ValueError(f"Router logits layer {i} contains Inf values")

            print(f"  ✓ All router logits valid (no NaN/Inf)")
        else:
            print(f"  ⚠ Router logits not available (may need output_router_logits=True in config)")

        # Output statistics
        print(f"  ✓ Forward pass successful")
        print(f"  ✓ Output shape: {outputs.logits.shape}")
        print(f"  ✓ Output range: [{outputs.logits.min().item():.2f}, {outputs.logits.max().item():.2f}]")
        print(f"  ✓ Output mean: {outputs.logits.mean().item():.4f}")
        print(f"  ✓ Output std: {outputs.logits.std().item():.4f}")

        # Sanity check: output should be in reasonable range for logits
        # Typical pre-softmax logits are in range [-20, 20] for language models
        if outputs.logits.abs().max() > 100:
            print(f"  ⚠ WARNING: Output values are very large (max abs: {outputs.logits.abs().max().item():.2f})")
            print(f"    This may indicate weight initialization issues")

        validation_passed = True

    except Exception as e:
        print(f"  ✗ Forward pass validation failed: {e}")
        import traceback
        traceback.print_exc()
        print("\n" + "="*80)
        print("VALIDATION FAILED - Model may not be usable")
        print("="*80)
        print("Possible issues:")
        print("  1. Incorrect weight copying (check shape assertions)")
        print("  2. Wrong weight format interpretation (interleaved vs concatenated)")
        print("  3. Incompatible shared expert configuration")
        print("  4. Device/memory issues")
        print("\nConversion will abort. Fix errors and try again.")
        print("="*80)
        return

    # Check for dry-run mode
    if args.dry_run:
        print("\n" + "="*80)
        print("DRY RUN MODE - Conversion successful but no files will be saved")
        print("="*80)
        print("Model validation passed. The conversion would succeed.")
        print(f"Output would be saved to: {args.output}")
        print("\nTo actually save the model, run without --dry-run flag.")
        print("="*80)
        return

    # Verify output directory is writable before expensive save operation
    print(f"\nPreparing output directory: {args.output}")
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)

    # Test write permissions
    test_file = output_path / ".write_test"
    try:
        test_file.touch()
        test_file.unlink()
        print("  ✓ Output directory is writable")
    except PermissionError:
        print(f"\n✗ ERROR: No write permission for {output_path}")
        print("  Please check directory permissions and try again")
        return
    except Exception as e:
        print(f"  ⚠ Warning: Could not verify write permissions: {e}")
        print("    Proceeding anyway...")

    # Save modified model
    print(f"\nSaving modified model...")

    model.save_pretrained(args.output)

    if tokenizer is not None:
        tokenizer.save_pretrained(args.output)
        print("  ✓ Tokenizer saved")
    else:
        print("  ⚠ Skipping tokenizer (not loaded)")

    # Save enhanced conversion metadata
    metadata = {
        "conversion_info": {
            "input_model": args.input,
            "router_stats": args.router_stats,
            "init_strategy": args.init_strategy,
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "script_version": "2.0",  # Incremented after comprehensive refactoring
        },
        "model_architecture": {
            "num_layers": model.config.num_hidden_layers,
            "num_shared_experts": args.num_shared_experts,
            "shared_expert_intermediate_size": model.config.shared_expert_intermediate_size,
            "original_num_experts": model.config.num_local_experts,
            "original_top_k": model.config.num_experts_per_tok,
            "hidden_size": model.config.hidden_size,
            "vocab_size": model.config.vocab_size,
        },
        "quantization": {
            "routed_experts": config_json.get("quantization_config", {}).get("quant_method", "none") if 'config_json' in locals() else "unknown",
            "shared_expert": "bfloat16",
        },
        "environment": {
            "pytorch_version": torch.__version__,
            "transformers_version": transformers.__version__,
            "device": str(args.device),
            "cuda_available": torch.cuda.is_available(),
        },
        "validation": {
            "forward_pass": validation_passed,
            "output_shape_correct": validation_passed,
            "no_nan_inf": validation_passed,
        },
    }

    # Add router stats summary if used
    if router_stats:
        metadata["router_stats_summary"] = {
            "num_layers": router_stats["num_layers"],
            "num_experts": router_stats["num_experts"],
            "top_k": router_stats.get("top_k"),
        }

    metadata_path = output_path / "conversion_metadata.json"
    with open(metadata_path, "w") as f:
        json.dump(metadata, f, indent=2)

    print("\n" + "=" * 80)
    print("Conversion Complete!")
    print("=" * 80)
    print(f"Modified model saved to: {args.output}")
    print(f"Metadata saved to: {metadata_path}")
    print("\nModel configuration:")
    print(f"  Routed experts: {model.config.num_local_experts}")
    print(f"  Shared experts: {model.config.num_shared_experts}")
    print(f"  Top-k per token: {model.config.num_experts_per_tok}")
    print(f"  Estimated activation: ~4.5B params per forward pass")
    print("=" * 80)


if __name__ == "__main__":
    main()
